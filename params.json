{"name":"Machinelearningcoursera","tagline":"","body":"# **Practical Machine Learning - Assignment report**\r\n\r\nThe assignment is asking us to predict the manner in which participant did an exercise - the \"classe\" variable in the training set\r\n\r\nI first read the testing and training data:\r\n\r\n> training<- read.csv(\"pml-training.csv\", na.strings=c(\"NA\",\" \",\"#DIV/0!\"))\r\n\r\n> testing<- read.csv(\"pml-testing.csv\", na.strings=c(\"NA\",\" \",\"#DIV/0!\"))\r\n\r\nFrom visual inspection of the training data I noticed that many of the input fields are actually NA, blank or equal to \"#DIV/0!”, therefore I used these as NA while reading the files, and then removed the unused columns\r\n\r\n> training<-training[, colSums(is.na(training)) != nrow(training)]\r\n\r\nFrom inspecting the data I also noticed that rows with the parameter new_window = “yes” are rare and the data in them looks completely different, therefor I decided to remove these lines from the training data as well\r\n\r\n> training<-training[training$new_window ==\"no\",] \r\n\r\nSome more inspection indicated that the first few columns are less relevant therefore I removed them as well\r\n\r\n> training<-subset( training, select = -c(X,user_name,raw_timestamp_part_1,raw_timestamp_part_2,cvtd_timestamp,new_window))\r\n\r\nI then created a training and testing subset in the data:\r\n\r\n> inTrain <- createDataPartition(y=training$classe,p=0.7, list=FALSE)\r\n\r\n> traininggroup <- training[inTrain,]\r\n\r\n> testinggroup <- training[-inTrain,]\r\n\r\nI then used the Caret package and its Train function, using Random Forest algorithm to predict the Classe parameter\r\n\r\n> tr1<-train(traininggroup$classe ~ ., data = traininggroup, method = \"rf\") \r\n\r\nThe result were as follows:\r\n\r\n> Random Forest \r\n\r\n> 13453 samples\r\n>    54 predictor\r\n>     5 classes: 'A', 'B', 'C', 'D', 'E' \r\n\r\n> No pre-processing\r\n> Resampling: Bootstrapped (25 reps) \r\n> Summary of sample sizes: 13453, 13453, 13453, 13453, 13453, 13453, ... \r\n> Resampling results across tuning parameters:\r\n\r\n>   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD\r\n   \r\n>    2    0.9911399  0.9887894  0.001793225  0.002270393\r\n\r\n>   28    0.9956940  0.9945522  0.001316920  0.001665525\r\n\r\n>   54    0.9919410  0.9898052  0.002631727  0.003324420\r\n\r\n> Accuracy was used to select the optimal model using  the largest value.\r\n> The final value used for the model was mtry = 28. \r\n\r\nI used the (internal) testing group to validate the model:\r\n\r\n> confusionMatrix(testinggroup$classe, predict(tr1, testinggroup))\r\n\r\nWhich resulted with the following accuracy of 0.9986:\r\n\t\t\r\n>                                           \r\n>                Accuracy : 0.9986          \r\n>                  95% CI : (0.9973, 0.9994)\r\n>     No Information Rate : 0.2847          \r\n>     P-Value [Acc > NIR] : < 2.2e-16       \r\n>                                           \r\n>                   Kappa : 0.9982          \r\n>  Mcnemar's Test P-Value : NA              \r\n\r\n\r\nFinally I used the prediction model on the test cases\r\n\r\n> predict(tr1, testing)\r\n\r\nAnd got the following results\r\n> [1] B A B A A E D B A A B C B A E E A B B B\r\n> Levels: A B C D E\r\n\r\nFurther analyzing the result I was looking for the most relevant parameters in the model:\r\n\r\n> varImp(tr1)\r\n\r\nWhich had the following results:\r\n\r\n> rf variable importance\r\n\r\n>   only 20 most important variables shown (out of 53)\r\n\r\n\r\n> num_window           100.000\r\n\r\n> roll_belt             60.683\r\n\r\n> pitch_forearm         36.901\r\n\r\n> yaw_belt              30.530\r\n\r\n> magnet_dumbbell_z     29.093\r\n\r\n> pitch_belt            28.035\r\n\r\n> magnet_dumbbell_y     27.036\r\n\r\n> roll_forearm          21.887\r\n\r\n> accel_dumbbell_y      12.423\r\n\r\n> roll_dumbbell         10.856\r\n\r\n> magnet_dumbbell_x     10.511\r\n\r\n> accel_forearm_x        9.847\r\n\r\n> accel_belt_z           9.292\r\n\r\n> total_accel_dumbbell   8.647\r\n\r\n> accel_dumbbell_z       7.412\r\n\r\n> magnet_belt_y          7.130\r\n\r\n> magnet_belt_z          6.792\r\n\r\n> magnet_forearm_z       6.601\r\n\r\n> magnet_belt_x          5.686\r\n\r\n> roll_arm               4.819\r\n\r\nI than draw the realtions between the most significant parameters:\r\n\r\n> featurePlot(x=training[,c(\"num_window\",\"roll_belt\",\"yaw_belt\",\"magnet_dumbbell_z\")], y = training$classe,plot=\"pairs\")\r\n ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}